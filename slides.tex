% -*- compile-command: "make slides.pdf" -*-
\documentclass{beamer}
\usepackage{tikz}
\usepackage[all]{xy}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{algorithmic}
\usepackage{multirow}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\Lik}{Lik}
\DeclareMathOperator*{\PoissonLoss}{PoissonLoss}
\DeclareMathOperator*{\Peaks}{Peaks}
\DeclareMathOperator*{\Segments}{Segments}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\RR}{\mathbb R}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\NN}{\mathbb N}

% Set transparency of non-highlighted sections in the table of
% contents slide.
\setbeamertemplate{section in toc shaded}[default][100]
\AtBeginSection[]
{
  \setbeamercolor{section in toc}{fg=red} 
  \setbeamercolor{section in toc shaded}{fg=black} 
  \begin{frame}
    \tableofcontents[currentsection]
  \end{frame}
}

% from https://tex.stackexchange.com/questions/160825/modifying-margins-for-one-slide
\newcommand\Wider[2][3cm]{%
\makebox[\linewidth][c]{%
  \begin{minipage}{\dimexpr\textwidth+#1\relax}
  \raggedright#2
  \end{minipage}%
  }%
}


\begin{document}

\title{Labeled Optimal Partitioning}

\author{
  Toby Dylan Hocking\\
  toby.hocking@nau.edu\\
  joint work with Anuraag Srivastava\\
  arXiv:2006.13967
}

%c\date{6 June 2020}

\maketitle

\section{Introduction: supervised changepoint detection for cancer diagnosis with DNA copy number data}

\begin{frame}
  \frametitle{Cancer cells show chromosomal copy number alterations}
  Spectral karyotypes show the number of copies of the sex chromosomes
  (X,Y) and autosomes (1-22). 

  Source: Alberts \emph{et al.} 2002.
\vskip 0.1in
  \includegraphics[width=\textwidth]{Karyo-both}
\vskip 0.1in
  \begin{minipage}{0.4\linewidth}
    Normal cell with 2 copies of each autosome.
  \end{minipage}
\hskip 0.1\linewidth
  \begin{minipage}{0.4\linewidth}
Cancer cell with many copy number alterations.
  \end{minipage}
\end{frame}

\begin{frame}
  \frametitle{DNA copy number profiles from neuroblastoma patients with or without relapse}

  \hspace*{-1cm}
  \includegraphics[width=1.15\textwidth]{figure-neuroblastoma-clinical}
  
\end{frame}

\begin{frame}
  \frametitle{Previous work: SegAnnDB interactive machine learning
    system}

  \includegraphics[width=\textwidth]{screenshot-SegAnnDB-figure-1}

  Hocking \emph{et al.}, 2014. 

  \begin{itemize}
  \item User uploads noisy data sets for machine learning analysis.
  \item User can provide labels which indicate presence(1) or
    absence(0) of changepoints in specific regions of data sets.
  \item Classic optimal changepoint model (max penalized Gaussian
    likelihood) used if it has zero label errors. OPART algorithm,
    Jackson \emph{et al.}, 2005. FPOP algorithm, Maidstone \emph{et
      al.}, 2016. 
  \item Label-aware SegAnnot algorithm used otherwise (Hocking and
    Rigaill, 2012). Always zero train label errors, but never predicts
    any changepoints outside of positive(1) labels.
  \end{itemize}
  
\end{frame}

\begin{frame}
  \frametitle{Example noisy data sequence}

  \includegraphics[width=\textwidth]{figure-baselines-data} 
  
\end{frame}

\begin{frame}
  \frametitle{Example noisy data sequence with labels}

  \includegraphics[width=\textwidth]{figure-baselines-labels}  
  
\end{frame}

\input{figure-baselines}

\section{New Labeled Optimal Partitioning (LOPART) Algorithm}

\begin{frame}
  \frametitle{Geometric interpretation of data and labels}
  Assume $N$ data and $M$ labels defined by
  \begin{itemize}
  \item $\mathbf x = [ x_1 \cdots x_N ]$ is the sequence of $N$ data,
  \item $  1 \leq 
\underline p_1 < \overline p_1 \leq  
\cdots \leq 
\underline p_M < \overline p_M \leq 
N$ are region start/ends,
  \item $y_j\in\{0,1\}$ is the number of changes expected in the region.
  \end{itemize}

  \input{figure-signal}   
  
\end{frame}

\begin{frame}
  \frametitle{Baseline/previous OPART algorithm}

  Assume
  \begin{itemize}
  \item $\mathbf x = [ x_1 \cdots x_N ]$ is the sequence of $N$ data,
  \item $\ell$ is a loss function (e.g. square loss),
  \item $I$ the indicator function counts the number of changes,
  \item $\lambda$ is a non-negative penalty (larger for fewer changes).
  \end{itemize}
  Then the problem and algorithm are
  \begin{eqnarray*}
    \label{eq:op}
     C_N &=& \min_{\mathbf m\in \mathbb R^N}
                 \sum_{i=1}^N \ell(m_i, x_i) + 
                 \lambda \sum_{i=1}^{N-1} I[m_i \neq m_{i+1}].\\
             &=& \min_{\tau\in \{0, 1, \dots, N-1\} }
                  C_\tau +
                 \lambda +
                 L(\tau+1, N, \mathbf x).
                 \label{eq:op-update}
  \end{eqnarray*}
  where
  \begin{itemize}
  \item $\tau$ is the last changepoint optimization variable,
  \item $ C_\tau$ is the optimal cost computed in previous iteration $\tau$,
  \item $L$ is the cost of the last segment.
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Proposed LOPART optimization problem}

  Assume $M$ labels are defined by
  \begin{itemize}
  \item $  1 \leq 
\underline p_1 < \overline p_1 \leq 
\cdots \leq 
\underline p_M < \overline p_M \leq 
N$ are region start/ends,
  \item $y_j\in\{0,1\}$ is the number of changes expected in the region.
  \end{itemize}
  The problem we want to solve is
\begin{align}
 \min_{
  \mathbf m\in\mathbb R^{N}
  } &\ \ 
  \nonumber
\sum_{i=1}^{N} 
    \ell(m_i, x_i) 
    +\lambda
 \sum_{i=1}^{N-1}
    I[m_i \neq m_{i+1}].
\\
  \alert{\text{subject to} }
    &
      \alert{\ \ \text{ for all } j\in\{1,\dots,M\},\,
y_j = \sum_{i=\underline p_j}^{\overline p_j-1}
    I[m_i \neq m_{i+1}].}
\nonumber
\end{align}

Objective function in black same as previous/unlabeled problem,\\
\alert{constraint in red new to proposed/labeled problem}.

\end{frame}

\begin{frame}
  \frametitle{Proposed LOPART dynamic programming algorithm}
Define the set of possible last changepoints at time $t$ as
\begin{equation*}
    T_t = \begin{cases}
      \alert{T_{t-1}} & \alert{
        \text{ if } 
        \exists j: y_j=0 \text{ and } 
        t\in \{\underline p_j + 1, \dots, \overline p_j \}
      }
      \\
      \alert{T_{t-1}} & \alert{
        \text{ if } 
        \exists j: y_j=1 \text{ and } 
        t\in \{\underline p_j + 1, \dots, \overline p_j - 1\}
      }
      \\
      \alert{\{\underline p_j, \dots, t-1\}} & \alert{
        \text{ if } 
        \exists j: y_j=1 \text{ and } 
        t= \overline p_j
      }
      \\
    T_{t-1}\cup \{t-1\} & \text{ otherwise. (unlabeled region)} 
    \end{cases}
  \end{equation*}
Update rule in black same as previous/unlabeled problem,\\
\alert{rules in red for labeled regions new to proposed problem}.
  
Then the optimal cost can be computed via
\begin{equation*}
  \tau^*_t,\,  W_t = \argmin,\, \min_{\tau \in \alert{T_t}}  W_\tau + \lambda + L(\tau+1, t, \mathbf x).
  \end{equation*}
(see paper for proof)

\end{frame}

\section{Demonstration of LOPART on example labeled data}

\input{figure-candidates}
 
\begin{frame}
  \frametitle{Interactive version, new algorithm requires less computation}
  Try this at home: click to show model/cost for any last data/change.

  \includegraphics[width=\textwidth]{screenshot-LOPART-interactive}

  \url{http://ml.nau.edu/viz/2021-07-23-LOPART-candidates/}
\end{frame}

\section{Results and Discussion}

\begin{frame}
  \frametitle{Empirical time complexity (labels)}
  \input{figure-timings-labels}

  Random normal data simulations.
\end{frame}

\begin{frame}
  \frametitle{Empirical time complexity (data)}
  \input{figure-timings}

  Random normal data simulations.
\end{frame}

\begin{frame}
  \includegraphics[width=\textwidth]{figure-label-errors-SegAnnot}

  Real genomic data labeled by biologists (Hocking
  \emph{et al.}, 2014).
  \begin{itemize}
  \item 413 sequences, $K=2$ fold cross-validation over labels.
  \item Data per sequence $N=39$ to 43628.
  \item Labels per sequence $M=2$ to 12.
  \end{itemize}
\end{frame}

\begin{frame}
  \includegraphics[width=\textwidth]{figure-label-errors}
\end{frame}
 
\begin{frame}
  \includegraphics[width=\textwidth]{figure-label-errors-BinSeg}
\end{frame}

\begin{frame}
  \frametitle{Test accuracy in cross-validation experiments}
  \includegraphics[width=\textwidth]{figure-cv-BIC}

  Penalty $\lambda$ predicted on test set using either unsupervised or
  supervised (constant/linear) methods:

\begin{description}
\item[linear.2] linear $\log \lambda_i = b + w \log\log N_i$ (Hocking
  2013), convex optimization of $w,b$ (supervised, 2 learned
  parameters).
\item[constant.1] grid search, constant $\lambda$ (supervised, 1
  learned parameter).
\item[BIC.0] classical Bayesian Information Criterion (Schwarz 1978),
  $\lambda_i=\log N_i$ (unsupervised, 0 learned parameters).
\end{description}
  
\end{frame}

\begin{frame}
  \frametitle{Test ROC curves in cross-validation experiments}
  \includegraphics[width=\textwidth]{figure-cv-BIC-roc}

  Dot = FPR/TPR at predicted threshold.
  
\end{frame}

\begin{frame}
  \frametitle{Summary and Discussion}
  \begin{itemize}
  \item Proposed algo fixes issues with two previous algorithms (better
  train AND test accuracy).
\item Results demonstrate improved speed and accuracy.
\item R package on CRAN and \url{https://github.com/tdhock/LOPART}
  \item Figure/slide code on \url{https://github.com/tdhock/LOPART-paper}
\item Future work: functional pruning algorithm, which can solve more
  complex constrained changepoint problems (e.g. change must be
  non-decreasing), and should be faster (log-linear instead of
  quadratic).
  \end{itemize}
\end{frame}

\end{document}
